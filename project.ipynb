{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inkompotato/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, RobertaForQuestionAnswering, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    name: str\n",
    "    tokenizer: object\n",
    "    model: object\n",
    "\n",
    "    def __init__(self, name: str, tokenizer: object = None, model: object = None):\n",
    "        self.name = name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Model(name={self.name})\"\n",
    "\n",
    "    def set_tokenizer(self, tokenizer: object):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_model(self, model: object):\n",
    "        self.model = model\n",
    "\n",
    "    def to_pipeline(self):\n",
    "        return pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0, top_k = 1)\n",
    "\n",
    "    def get_collator(self):\n",
    "        return DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "\tdef __init__(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number =0\n",
    "\t\tself.str = 'perplexity'\n",
    "\t\tself.metric_scores = {}\n",
    "\n",
    "\t# cross entropy loss\n",
    "\tdef score(self, loss):\n",
    "\t\tself.sum += loss\n",
    "\t\t# should be number of sentences\n",
    "\t\tself.number += 1\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number = 0\n",
    "\n",
    "\tdef get_score(self):\n",
    "\t\tif self.sum ==0:\n",
    "\t\t\tself.metric_scores[self.str] = 0.0\n",
    "\t\tself.metric_scores[self.str] = math.exp(self.sum / self.number)\n",
    "\t\tself.metric_scores[\"sum\"] = self.str\n",
    "\t\treturn self.metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:25<00:00,  5.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "models = {\n",
    "    \"de\" : Model(\"uklfr/gottbert-base\"),\n",
    "    \"nl\" : Model(\"pdelobelle/robbert-v2-dutch-base\"),\n",
    "    \"es\" : Model(\"bertin-project/bertin-roberta-base-spanish\"),\n",
    "    \"se\" : Model(\"birgermoell/roberta-swedish\"),\n",
    "    \"dk\" : Model(\"DDSC/roberta-base-danish\"),\n",
    "}\n",
    "\n",
    "for language, model in tqdm(models.items()):\n",
    "    # get model & tokenizer from huggingface\n",
    "    model.set_tokenizer(AutoTokenizer.from_pretrained(model.name, model_max_length=512))\n",
    "    model.set_model(AutoModelForMaskedLM.from_pretrained(model.name).to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines\n",
    "\n",
    "pipelines = {k: v.to_pipeline() for k, v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load datasets\n",
    "\n",
    "def load_dataset_local(dataset_name):\n",
    "    with open(f\"wiki_data/{dataset_name}.txt\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "datasets = {\n",
    "    \"dk\" : load_dataset_local(\"dawiki_sentences\"),\n",
    "    \"nl\" : load_dataset_local(\"nlwiki_sentences\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:07<00:00, 100.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - dk: 0.2492884303535838, 0.1815131002795862, 12433.975734226316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:05<00:00, 131.07it/s]\n",
      " 20%|██        | 1/5 [02:41<10:46, 161.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - nl: 0.28904147550884396, 0.027586013242075533, 123330429.02641025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:05<00:00, 139.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - dk: 0.17430659572121546, 0.17302126693456454, 729.0852189691547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:02<00:00, 267.83it/s]\n",
      " 40%|████      | 2/5 [05:02<07:28, 149.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - nl: 0.5049835043971108, 0.12403929017687482, 341670703.0492279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with es and dk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:04<00:00, 157.21it/s]\n",
      " 60%|██████    | 3/5 [07:07<04:36, 138.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es - nl: 0.25532941688059496, 0.029381318419371947, 65217211.173626855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:03<00:00, 182.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - dk: 0.31381282833459184, 0.21487827390014422, 883890.0907943965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:04<00:00, 148.60it/s]\n",
      " 80%|████████  | 4/5 [09:35<02:22, 142.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - nl: 0.25506704479875, 0.02644637501273703, 102829901.5122894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [09:40<02:25, 145.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/inkompotato/itu/nlp-project/project.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bskybrain/home/inkompotato/itu/nlp-project/project.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bskybrain/home/inkompotato/itu/nlp-project/project.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bskybrain/home/inkompotato/itu/nlp-project/project.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     predictions \u001b[39m=\u001b[39m pipeline(sentences)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bskybrain/home/inkompotato/itu/nlp-project/project.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bskybrain/home/inkompotato/itu/nlp-project/project.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError with \u001b[39m\u001b[39m{\u001b[39;00mmodel_language\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mdataset_language\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:225\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    204\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m        - **token** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    227\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1012\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1013\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[0;32m-> 1015\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1015\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1012\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1013\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[0;32m-> 1015\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:112\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/fill_mask.py:116\u001b[0m, in \u001b[0;36mFillMaskPipeline.postprocess\u001b[0;34m(self, model_outputs, top_k, target_ids)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m# Fill mask pipeline supports only one ${mask_token} per sample\u001b[39;00m\n\u001b[1;32m    115\u001b[0m logits \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m, masked_index, :]\n\u001b[0;32m--> 116\u001b[0m probs \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39;49msoftmax(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m target_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     probs \u001b[39m=\u001b[39m probs[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, target_ids]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# tokenize each dataset for each model\n",
    "for model_language, pipeline in tqdm(pipelines.items()):\n",
    "\n",
    "    for dataset_language, dataset in datasets.items():\n",
    "        # raw data\n",
    "        raw_data = dataset\n",
    "        # split into sentences\n",
    "        # TODO: permorm additional data cleaning\n",
    "        sentences = [sentence.split(\" \") for text in raw_data for sentence in text.split(\".\")]\n",
    "        masked_words = []     \n",
    "        # mask one word in each sentence\n",
    "        for sentence in sentences:\n",
    "            # get random index\n",
    "            index = random.randint(0, len(sentence) - 1)\n",
    "            # mask word\n",
    "            masked_words.append(sentence[index])\n",
    "            sentence[index] = pipeline.tokenizer.mask_token\n",
    "\n",
    "        sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline(sentences)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error with {model_language} and {dataset_language}\")\n",
    "            continue\n",
    "\n",
    "        # get average score\n",
    "        softmax_scores = [prediction[0][\"score\"] for prediction in predictions]\n",
    "        predicted_sentences = [prediction[0][\"sequence\"] for prediction in predictions]\n",
    "        predicted_words = [prediction[0][\"token_str\"] for prediction in predictions]\n",
    "        # get vector for masked and predicted word\n",
    "        masked_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=False) for token in masked_words]\n",
    "        predict_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=False) for token in predicted_words]\n",
    "        # calculate cosine similarity between predicted words and masked words\n",
    "        cosine_similarities = []\n",
    "        for i in range(len(masked_index)):\n",
    "            if masked_index[i] == [] or predict_index[i] == []:\n",
    "                cosine_similarities.append(np.array([0]))\n",
    "                continue\n",
    "            # get vector for masked word\n",
    "            masked_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(masked_index[i]).to(\"cuda\"))\n",
    "            # get vector for predicted word\n",
    "            predict_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(predict_index[i]).to(\"cuda\"))\n",
    "            # fill the smaller tensor with zeros so that masked_vector and predict_vector have the same shape\n",
    "            if masked_vector.shape[0] < predict_vector.shape[0]:\n",
    "                masked_vector = torch.nn.functional.pad(masked_vector, (0, 0, 0, predict_vector.shape[0] - masked_vector.shape[0]))\n",
    "            elif masked_vector.shape[0] > predict_vector.shape[0]:\n",
    "                predict_vector = torch.nn.functional.pad(predict_vector, (0, 0, 0, masked_vector.shape[0] - predict_vector.shape[0]))\n",
    "\n",
    "            # calculate cosine similarity using torch\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(masked_vector, predict_vector, dim=1).cpu().detach().numpy()\n",
    "            cosine_similarities.append(cos_sim)\n",
    "\n",
    "        # calculate perplexity\n",
    "        perplexity_scores = perplexity.compute(predictions=predicted_words, references=masked_words, model_id=models[dataset_language].name)\n",
    "        perplexity_score = np.mean(perplexity_scores[\"perplexities\"]) / len(masked_words)\n",
    "        # mean cosine similarity\n",
    "        # flatten cos sim\n",
    "        cosine_similarities = [item for sublist in cosine_similarities for item in sublist]\n",
    "        mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "        average_score = np.mean(softmax_scores)\n",
    "        median_score = np.median(softmax_scores)\n",
    "        print(f\"{model_language} - {dataset_language}: {average_score}, {mean_cosine_similarity}, {perplexity_score}\")\n",
    "        results[(model_language, dataset_language)] = [average_score, median_score, mean_cosine_similarity, perplexity_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.0848e-02, -1.3256e-01,  7.0148e-02, -6.4970e-02,  3.9045e-02,\n",
       "          -1.6804e-01,  4.8733e-02, -1.3285e-02,  9.7881e-02, -1.2284e-01,\n",
       "           1.2004e-01, -1.3028e-01, -4.7270e-02,  7.0994e-03, -1.3049e-01,\n",
       "           2.9180e-03, -2.1014e-03,  5.3837e-03, -8.0522e-02, -9.2563e-03,\n",
       "          -2.4973e-03,  2.6751e-02, -4.8266e-02, -2.9054e-02, -3.5251e-02,\n",
       "           3.6251e-02,  1.2799e-02,  5.3483e-02, -7.4835e-03,  9.3621e-02,\n",
       "           1.1026e-02,  4.2390e-02,  2.5491e-03, -7.8009e-02,  3.2994e-02,\n",
       "           2.6747e-02, -5.3774e-02,  7.7423e-02,  2.1110e-02, -1.4736e-01,\n",
       "          -3.7681e-02, -1.5344e-01,  1.0016e-01, -1.7052e-01, -2.1911e-01,\n",
       "          -8.6054e-02,  8.1558e-02, -6.2331e-02, -2.3602e-02, -3.4057e-02,\n",
       "           2.3995e-02, -1.8137e-02, -8.1614e-03, -3.5421e-02,  9.7237e-02,\n",
       "          -1.0874e-01, -7.5366e-02,  3.4162e-02, -2.9242e-02, -5.2934e-02,\n",
       "           1.2119e-02, -1.0767e-01, -8.3322e-02, -3.4783e-02,  2.8931e-02,\n",
       "          -4.3873e-02, -2.4386e-03, -4.4165e-02, -6.4109e-02, -1.1193e-01,\n",
       "           1.2464e-01, -1.6004e-01, -3.4255e-02,  1.8308e-03, -1.3334e-01,\n",
       "          -1.4218e-01, -5.0551e-02, -2.8439e-02,  2.8839e-02, -1.2448e-01,\n",
       "           6.5468e-02, -4.2480e-03,  1.6277e-01, -4.9271e-02, -5.0686e-02,\n",
       "          -1.1964e-01, -1.4361e-01, -4.5364e-02, -1.1991e-02,  9.2830e-02,\n",
       "          -1.7227e-03,  8.8393e-02, -1.0244e-01,  7.8766e-02, -1.2159e-03,\n",
       "           9.6878e-02,  2.8721e-02, -1.2233e-01, -3.1047e-02, -7.1704e-02,\n",
       "          -6.2910e-02,  3.2642e-02, -5.6825e-03,  3.6057e-02,  5.3971e-02,\n",
       "           8.2740e-02,  1.0835e-01,  3.4671e-02, -4.9337e-02,  7.4985e-02,\n",
       "           8.1937e-02,  5.0121e-02,  6.0758e-02,  8.0159e-02, -5.1614e-02,\n",
       "          -5.9043e-02, -6.2922e-02,  3.8220e-02,  7.7562e-02, -1.0227e-02,\n",
       "          -8.3341e-02,  2.3332e-02, -7.0409e-02,  3.7279e-03,  1.1408e-02,\n",
       "          -1.2722e-02, -2.1086e-02,  5.8379e-02, -7.5544e-02,  1.1596e-02,\n",
       "           1.6812e-02,  6.7915e-02,  1.1850e-01, -3.1621e-02,  5.6437e-02,\n",
       "          -9.5182e-02, -2.1098e-02, -8.5323e-02,  4.4172e-02, -1.0509e-01,\n",
       "          -1.8101e-01,  7.1264e-03, -1.2779e-01, -4.5870e-02,  6.5683e-02,\n",
       "           1.3835e-01,  5.4610e-03, -7.2870e-02,  9.6598e-02, -7.1570e-02,\n",
       "          -5.0449e-02,  1.1967e-01,  6.3448e-02, -1.0858e-02, -1.5578e-02,\n",
       "          -1.4846e-02, -8.1755e-02, -7.7899e-05, -2.1082e-02,  2.8001e-02,\n",
       "          -1.2285e-01,  2.7286e-02, -4.6354e-02, -1.1788e-01, -8.5702e-02,\n",
       "          -4.2467e-02, -4.2175e-03, -6.5533e-02,  1.6854e-01, -5.9850e-02,\n",
       "           3.4794e-02, -1.0823e-02, -9.8158e-02, -1.2087e-02,  1.8012e-02,\n",
       "          -3.1327e-02, -8.9758e-02,  7.2006e-02, -1.1148e-01,  6.4914e-02,\n",
       "           1.9053e-02, -4.4416e-02,  4.1545e-02,  1.3763e-01,  1.2381e-02,\n",
       "          -2.0449e-02,  2.1690e-02,  1.1253e-01, -4.8286e-02,  4.9028e-03,\n",
       "          -1.3519e-02,  1.9078e-02, -6.5624e-02, -4.0514e-02, -1.6922e-01,\n",
       "           1.1716e-02, -3.0110e-02, -1.5106e-01, -8.1273e-02, -5.2918e-02,\n",
       "          -1.0257e-02, -8.2936e-02,  2.7645e-02,  1.5696e-02,  1.4665e-01,\n",
       "           5.2388e-03, -1.1419e-01, -2.5432e-02,  1.7855e-02, -5.0348e-02,\n",
       "          -7.2423e-02, -6.7559e-02,  9.0791e-04,  3.3538e-02,  3.1569e-02,\n",
       "          -9.8127e-02, -1.1336e-02, -1.2727e-02, -7.2830e-02, -4.3843e-02,\n",
       "           7.8661e-02, -2.9537e-02,  2.4125e-02, -6.4630e-02, -1.1675e-01,\n",
       "           1.3854e-01, -1.2287e-01, -3.8324e-03, -1.0898e-03,  1.8454e-02,\n",
       "          -2.8618e-03, -2.4351e-03, -1.4840e-01,  5.2326e-02, -7.6082e-02,\n",
       "          -3.2646e-02, -2.8619e-02,  3.5508e-02, -9.9370e-02,  8.3286e-02,\n",
       "           1.3937e-01,  1.8049e-02,  3.8867e-02,  5.1783e-03, -4.9110e-02,\n",
       "           6.2192e-02, -8.3733e-02,  6.5924e-02, -1.3051e-01,  2.0209e-02,\n",
       "           9.4662e-04, -4.6554e-02,  8.0742e-03,  9.6524e-02, -6.9899e-02,\n",
       "          -4.6887e-02, -6.0136e-02,  9.2704e-02,  1.7341e-01, -1.3271e-01,\n",
       "           1.8976e-02,  8.9587e-03,  4.2553e-02,  1.8073e-02, -9.7196e-02,\n",
       "          -8.2179e-03, -1.6520e-01,  1.1868e-01,  4.2922e-03, -1.0352e-01,\n",
       "          -3.9176e-02,  9.1282e-03, -1.2142e-01,  5.7436e-03, -8.9037e-02,\n",
       "           3.4928e-02,  5.5743e-02,  3.1283e-02, -9.4757e-02,  9.2863e-02,\n",
       "          -8.8139e-02,  2.7939e-02,  3.0212e-02,  1.5952e-02, -1.7874e-01,\n",
       "          -1.4614e-02, -2.7056e-02, -3.4176e-02,  4.1444e-02, -3.9187e-02,\n",
       "           1.0672e-01, -6.5032e-02,  3.0209e-02,  1.0958e-01,  3.6252e-02,\n",
       "           2.3744e-02,  4.7708e-02, -5.3951e-02,  1.3090e-02, -1.1855e-01,\n",
       "          -1.5995e-02, -2.0075e-02, -4.3008e-02,  4.8236e-02, -6.2637e-02,\n",
       "           1.1098e-01, -3.4666e-02, -1.3737e-01,  6.6350e-02,  7.6336e-03,\n",
       "           3.6922e-02, -1.5121e-02, -1.5985e-02, -2.5963e-02,  1.0097e-01,\n",
       "           6.7838e-02, -4.4307e-02, -1.5309e-02, -2.4280e-02, -5.7126e-02,\n",
       "           3.7071e-02, -2.0033e-02, -2.4394e-02,  1.6712e-01,  3.9455e-02,\n",
       "           3.6422e-02, -5.4804e-02,  8.9303e-02,  4.2284e-02,  7.7740e-02,\n",
       "           8.9204e-03, -6.0251e-02, -1.9673e-02, -2.3702e-02,  2.4564e-02,\n",
       "          -3.3312e-02, -1.0747e-01,  2.8636e-03,  2.9946e-02,  2.6028e-02,\n",
       "           6.4714e-02,  8.1202e-02,  5.1760e-02,  2.5023e-02,  9.1096e-02,\n",
       "          -9.7392e-02, -1.3494e-02, -7.0769e-02,  1.1656e-02, -9.4049e-03,\n",
       "          -1.9044e-01, -4.5747e-02, -6.6231e-02, -1.3169e-02, -6.8607e-02,\n",
       "          -9.2271e-02,  2.5090e-02, -5.2266e-02,  4.4470e-03, -1.1420e-01,\n",
       "          -4.9851e-03, -3.5394e-02, -1.1678e-02,  4.6681e-03, -1.5342e-02,\n",
       "          -1.5681e-02,  7.2815e-03, -6.1569e-02, -8.4901e-02, -5.2261e-02,\n",
       "           9.1372e-02, -4.4256e-02, -3.1285e-02,  7.7075e-02, -1.6852e-02,\n",
       "          -2.0152e-02,  2.3091e-02,  6.0193e-03,  9.6627e-02,  7.2696e-03,\n",
       "           3.4735e-02, -1.6965e-02,  3.2807e-02, -5.5024e-03,  3.4922e-02,\n",
       "           2.2158e-02,  3.3583e-02, -9.6713e-02, -9.7025e-02, -5.8398e-02,\n",
       "           4.6506e-02, -6.8608e-02, -7.1112e-02, -3.0406e-02, -4.3458e-02,\n",
       "           8.1304e-03, -7.9478e-02, -6.2852e-02,  4.9604e-02, -3.4124e-02,\n",
       "           1.1858e-01,  2.0702e-01, -1.1476e-02,  2.2903e-02, -2.1977e-02,\n",
       "          -5.0548e-02, -4.9882e-02,  7.7783e-03, -7.6854e-02, -1.3622e-02,\n",
       "           1.1652e-01,  2.3788e-02,  9.4193e-03,  3.9059e-02,  3.0582e-02,\n",
       "           4.2867e-02,  2.0179e-01, -1.3672e-03, -5.6515e-02, -1.2163e-01,\n",
       "          -2.2037e-03, -1.5268e-02, -9.8817e-02, -4.0269e-02, -4.4078e-02,\n",
       "          -4.2907e-02, -1.4397e-02,  8.1697e-02, -4.9812e-02, -4.4204e-03,\n",
       "           5.6529e-02, -1.1615e-01, -1.5863e-01, -1.3737e-01,  5.3115e-03,\n",
       "           1.1593e-02, -1.0165e-02,  8.5057e-03,  3.0002e-02, -1.1705e-01,\n",
       "           6.4454e-02, -4.3779e-02, -6.4565e-02,  1.5312e-01, -6.7051e-02,\n",
       "          -2.0253e-01, -1.1916e-01, -7.8803e-03,  5.0308e-02, -1.5620e-01,\n",
       "          -7.2233e-02,  9.9938e-05, -1.0610e-01,  1.9434e-02,  7.7651e-02,\n",
       "           2.3581e-02,  1.1688e-02, -1.0952e-01, -3.5159e-02, -1.0727e-01,\n",
       "          -9.5487e-02, -7.1740e-02, -2.0284e-02,  8.2109e-02,  1.0367e-04,\n",
       "          -9.0266e-02, -2.4707e-02, -6.1779e-03, -2.6230e-02, -7.0601e-02,\n",
       "          -5.7711e-02, -5.8076e-02,  2.8286e-02,  3.7582e-03, -4.2219e-02,\n",
       "          -6.5263e-02,  1.2405e-01, -2.1987e-02, -9.8362e-03,  1.6282e-02,\n",
       "          -3.3402e-03, -1.9459e-02,  1.3610e-01,  4.2215e-02, -6.1929e-03,\n",
       "           4.4631e-02,  3.7777e-02, -8.5651e-02, -6.3989e-02, -2.0813e-01,\n",
       "           1.0297e-02,  9.3209e-02, -3.5681e-02,  6.6946e-03, -2.2750e-03,\n",
       "          -2.0290e-01,  1.5780e-02,  1.6752e-02,  3.1299e-02, -4.5076e-02,\n",
       "           3.9886e-02,  3.0535e-03,  4.1140e-02, -5.1647e-02, -9.6134e-02,\n",
       "          -3.7368e-02,  1.4098e-01,  3.5745e-02,  3.6264e-02, -1.1543e-02,\n",
       "          -1.9617e-02,  4.8320e-03,  5.7794e-02,  3.2446e-02, -1.0805e-01,\n",
       "           9.9433e-02, -3.1250e-02, -4.4881e-02,  8.4986e-02,  1.2619e-01,\n",
       "          -1.1421e-01, -9.3299e-02,  3.3243e-02,  2.8549e-02,  1.0250e-01,\n",
       "           8.1719e-03, -2.6267e-03, -8.1228e-03,  1.0478e-01, -9.3038e-02,\n",
       "          -1.7928e-01, -3.5153e-02, -6.9681e-02, -1.0119e-01, -7.0411e-02,\n",
       "           1.4830e-02,  6.7374e-02, -1.1977e-01, -2.2566e-02,  4.2765e-02,\n",
       "           3.1189e-02,  9.8588e-03,  1.0609e-02, -7.0636e-02, -1.2423e-01,\n",
       "          -7.0922e-02, -1.0305e-01,  1.4827e-03, -1.9343e-02,  8.5844e-02,\n",
       "           4.1935e-02, -3.6732e-02, -1.1069e-01, -2.2678e-02,  1.0487e-02,\n",
       "          -1.0428e-02, -5.3691e-02, -2.5732e-02,  2.2880e-02, -4.7210e-02,\n",
       "           1.0703e-01, -1.0395e-01, -1.1346e-01,  7.4425e-02, -5.5244e-02,\n",
       "          -5.6536e-02, -8.9611e-02, -2.1043e-03, -3.9080e-03, -1.2760e-02,\n",
       "          -5.3853e-02, -1.2891e-01,  2.8359e-02, -7.9679e-02, -5.6607e-02,\n",
       "          -1.0315e-01,  5.5243e-02,  1.7057e-02,  3.5352e-02, -1.7672e-02,\n",
       "          -1.7276e-01,  9.0154e-02,  1.6003e-01, -5.1632e-03, -3.6216e-02,\n",
       "          -1.1894e-01, -3.5996e-02, -2.4011e-02, -5.8600e-02, -4.2455e-02,\n",
       "           4.7040e-02, -4.0044e-04, -1.5446e-01,  3.8595e-02, -6.4482e-02,\n",
       "          -6.1213e-03,  1.8965e-02, -1.0707e-01,  9.8340e-02,  5.5625e-02,\n",
       "           6.5012e-02, -1.2486e-01, -3.0163e-02, -6.2400e-02, -1.4131e-02,\n",
       "          -9.3597e-02,  4.2135e-02, -5.5268e-02, -3.3274e-02, -2.9944e-02,\n",
       "           7.6201e-02,  8.2879e-02, -2.6907e-02, -1.3884e-01,  8.6110e-02,\n",
       "           3.2866e-02,  6.3876e-02,  3.5231e-02,  2.3533e-02, -3.6393e-02,\n",
       "          -1.2827e-01,  2.8784e-04, -3.9280e-03, -3.1389e-02, -3.8577e-02,\n",
       "          -3.2115e-02,  2.1323e-02, -4.9030e-02,  4.1431e-03,  4.9967e-02,\n",
       "           4.9411e-02,  8.0389e-03,  7.1816e-02, -1.5262e-02, -6.3146e-02,\n",
       "           3.8317e-02,  1.0657e-01,  1.4027e-03, -7.8568e-02, -1.0597e-01,\n",
       "           5.3613e-02,  5.3034e-02, -8.2356e-02, -1.0864e-01,  3.0670e-02,\n",
       "           1.7982e-04,  6.8490e-02,  4.7377e-02, -1.1094e-01,  2.1193e-02,\n",
       "           1.4450e-02,  3.0106e-02,  1.2986e-02, -7.5744e-02, -8.7073e-02,\n",
       "           5.9241e-02, -3.8114e-02,  2.0303e-02, -2.3652e-02, -6.5096e-02,\n",
       "           4.6130e-02,  1.3700e-01, -5.0850e-02,  6.8170e-02, -6.1530e-02,\n",
       "          -8.0189e-02,  4.7996e-02,  2.6746e-02,  1.9157e-02,  2.2255e-02,\n",
       "           8.2238e-03,  4.2101e-02, -3.0984e-02, -5.2431e-02, -6.7675e-02,\n",
       "           1.1194e-02,  5.5774e-02,  9.3175e-02, -9.6365e-02,  4.3242e-02,\n",
       "          -3.5538e-02,  4.6606e-02, -3.2659e-02, -7.8154e-02, -1.2037e-01,\n",
       "           7.2071e-02, -1.8065e-02, -1.6547e-02,  1.5094e-01,  1.2628e-02,\n",
       "          -1.1848e-01,  4.0868e-03,  5.1593e-03,  1.0104e-01, -6.0510e-02,\n",
       "           1.1261e-02, -6.3724e-02, -5.6013e-02,  8.2467e-03,  8.2462e-02,\n",
       "          -9.6477e-02, -1.6396e-01, -1.2052e-01,  3.0465e-02, -2.3318e-02,\n",
       "           1.3649e-01, -9.5936e-02,  1.0908e-01, -9.1051e-02, -1.4243e-01,\n",
       "           3.2861e-02, -1.6457e-02,  8.9164e-02, -3.1297e-02,  9.0892e-02,\n",
       "          -3.0181e-02,  1.5587e-02,  4.2886e-03, -6.2104e-02, -4.0534e-02,\n",
       "          -1.1713e-01, -1.9227e-02, -1.3471e-01, -5.4374e-02,  1.1265e-02,\n",
       "           6.3899e-02, -8.9035e-02, -5.3906e-02,  1.7740e-02, -2.7623e-02,\n",
       "           1.5506e-01,  6.8497e-03, -9.6106e-02, -3.5815e-02,  3.7895e-02,\n",
       "           6.0802e-02, -2.2112e-03,  6.8067e-03, -7.1186e-02, -4.8692e-02,\n",
       "           8.8985e-02, -5.6998e-02,  1.4141e-02, -4.0079e-02, -5.6020e-02,\n",
       "          -2.3426e-01, -1.1265e-02,  8.5385e-02,  7.2045e-02, -5.6151e-02,\n",
       "           6.0021e-02,  1.0234e-02, -7.4047e-03,  8.9024e-02,  1.4450e-01,\n",
       "          -2.6528e-02,  6.1095e-02,  1.6528e-02,  4.9147e-02, -1.1214e-01,\n",
       "          -6.8370e-02,  2.3586e-02, -1.4627e-01, -6.7368e-02, -1.7787e-02,\n",
       "           6.1824e-02, -1.1489e-01,  1.3157e-02]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[ 9.1735e-02,  2.1622e-02, -3.6267e-02,  1.2710e-01,  1.2558e-02,\n",
       "          -1.4247e-02, -5.2779e-02,  7.4919e-02, -3.3612e-02, -9.8327e-02,\n",
       "          -6.6333e-02,  8.4496e-03, -1.3299e-01,  4.1055e-02,  1.8433e-01,\n",
       "           6.7186e-02, -3.0841e-02, -5.6314e-02,  5.9660e-03,  4.7221e-02,\n",
       "          -3.0342e-02,  1.4002e-02, -3.6898e-02,  1.4136e-02,  9.1098e-03,\n",
       "           1.8881e-02, -6.0528e-02, -1.4036e-01, -3.3682e-03, -3.6669e-02,\n",
       "           3.8979e-02,  2.1738e-02, -3.1678e-02,  2.4886e-03, -4.4358e-02,\n",
       "          -3.7382e-02, -7.2189e-02,  9.7016e-02, -4.6241e-02, -1.6498e-01,\n",
       "           1.2413e-03,  4.9809e-02,  1.0786e-01, -1.2345e-01,  1.0390e-01,\n",
       "          -9.1605e-02,  3.7769e-02,  9.5466e-02, -6.7729e-02, -3.6210e-02,\n",
       "          -3.0256e-02,  6.6337e-02, -2.7000e-02,  6.3213e-02, -9.5080e-02,\n",
       "           6.2443e-02, -8.4869e-02,  1.2631e-03, -4.2320e-02, -5.2761e-03,\n",
       "           5.9537e-03, -5.7381e-02, -7.8093e-02, -5.6738e-02,  2.4434e-02,\n",
       "           9.2582e-02, -2.0076e-02, -3.0283e-03,  6.6332e-02,  5.2499e-02,\n",
       "          -1.4027e-01, -2.8998e-02, -6.3300e-02, -1.1535e-01, -4.5923e-03,\n",
       "           1.3497e-01, -3.2568e-02, -7.2158e-02, -4.9298e-02,  2.7154e-02,\n",
       "           1.0186e-01, -1.2075e-01,  2.6331e-02, -7.7426e-03, -5.1087e-02,\n",
       "          -2.3762e-02, -6.8581e-02, -7.4335e-02,  1.8081e-02, -2.1520e-02,\n",
       "          -3.2055e-02,  6.2783e-02, -5.0358e-02,  5.4086e-03, -4.5305e-02,\n",
       "           1.1618e-02,  6.8970e-02, -3.5887e-02, -2.0869e-02,  8.9970e-02,\n",
       "           2.0448e-02, -1.5820e-02, -3.3231e-03, -4.9473e-02,  3.6148e-02,\n",
       "          -1.4203e-01, -3.5330e-02,  3.3078e-02, -2.3384e-03,  5.3823e-02,\n",
       "          -5.7837e-02, -6.1822e-02,  3.3603e-02, -4.5549e-02,  1.4942e-02,\n",
       "          -2.1862e-02,  2.7116e-02, -9.4924e-03, -3.3616e-02, -7.5024e-02,\n",
       "           8.2628e-02, -2.5582e-02, -3.3112e-02,  6.1797e-02,  3.4048e-02,\n",
       "           3.7471e-02,  1.1322e-01, -1.6939e-01, -1.6852e-01, -2.0601e-02,\n",
       "          -2.4261e-02,  4.9625e-03, -8.8519e-03,  8.9574e-02,  4.6830e-03,\n",
       "          -4.4451e-02,  3.0590e-03, -5.9905e-02,  3.8041e-02,  9.0379e-03,\n",
       "           8.9250e-02,  8.9674e-02,  7.5154e-03,  1.0635e-01,  8.9461e-02,\n",
       "           6.3694e-03,  1.9777e-02,  4.2951e-02, -7.4069e-02, -7.3886e-02,\n",
       "          -5.8909e-02, -2.4512e-02, -4.5359e-02,  3.0081e-02,  5.1298e-02,\n",
       "           8.2204e-02, -3.0530e-03, -7.7935e-03,  3.9519e-02,  4.9396e-02,\n",
       "          -1.0296e-01, -6.9623e-05, -7.4991e-02, -7.0583e-02,  1.3752e-02,\n",
       "           6.6788e-02, -9.7341e-03,  6.9032e-02,  1.1550e-01,  3.4205e-02,\n",
       "          -1.3388e-01,  6.0983e-02,  3.3258e-02, -3.8645e-03,  1.4916e-01,\n",
       "          -6.6078e-02, -9.0086e-02,  9.0693e-02, -9.4940e-03, -2.7561e-02,\n",
       "           5.0082e-02, -2.0919e-02, -3.8610e-02,  5.8382e-02,  6.9696e-02,\n",
       "          -7.2980e-02,  6.8979e-02, -7.7535e-02, -7.7747e-02, -5.9519e-03,\n",
       "           3.7217e-02, -1.8084e-02, -5.7236e-02,  1.6088e-02,  3.8803e-02,\n",
       "           5.5631e-03, -8.9036e-03,  6.8989e-02,  7.4236e-02,  7.2549e-02,\n",
       "           2.3304e-03,  3.9737e-02, -2.3997e-02,  9.2721e-03, -1.7469e-02,\n",
       "           4.4462e-02, -7.6193e-02,  4.0663e-02, -6.5475e-03,  1.4324e-02,\n",
       "           1.7932e-01, -7.5026e-02, -8.3607e-02,  2.5821e-02,  6.3601e-02,\n",
       "          -4.9494e-03,  3.7950e-02,  2.2255e-02,  5.3745e-02, -4.6559e-02,\n",
       "          -6.4949e-02, -7.6807e-02, -4.4022e-02, -4.2290e-02,  7.9409e-02,\n",
       "           8.6347e-03,  1.7964e-01,  6.6666e-02, -1.0990e-02,  3.1770e-02,\n",
       "          -3.9835e-03, -9.8625e-02,  4.7892e-02, -1.7377e-02, -1.0862e-02,\n",
       "           7.9446e-03, -1.2785e-01,  8.8221e-04,  4.6657e-02, -7.0416e-02,\n",
       "          -3.0876e-02, -4.1341e-02,  1.3891e-02,  6.0314e-02,  1.0515e-01,\n",
       "          -3.6404e-02, -7.0544e-03, -1.2687e-02, -3.9837e-02,  1.2880e-02,\n",
       "          -4.0273e-02, -2.9618e-02,  1.5999e-01,  4.1211e-02, -8.6300e-02,\n",
       "           3.4592e-02, -1.3242e-01, -9.9719e-02,  1.5904e-01, -4.6974e-02,\n",
       "           4.6704e-02,  2.2279e-02, -7.6073e-02, -1.0944e-02, -1.7467e-02,\n",
       "          -3.7780e-02, -7.9014e-02, -6.6679e-02, -4.6378e-02, -2.5690e-02,\n",
       "          -1.1478e-01,  1.3658e-02, -6.5446e-02, -9.8505e-02, -4.9548e-03,\n",
       "           6.8869e-02, -2.8834e-02,  1.2398e-02, -7.9769e-02,  4.5035e-02,\n",
       "           1.4816e-01, -5.9834e-02,  3.6400e-02, -9.0125e-02, -1.2398e-01,\n",
       "           7.4800e-04,  7.1813e-02,  9.9057e-02, -1.6972e-02, -8.4152e-02,\n",
       "          -9.2954e-02,  6.4061e-02,  3.3785e-02,  7.4482e-02,  3.0799e-02,\n",
       "          -7.8407e-02, -2.5675e-03, -1.0248e-01,  5.8908e-02, -4.2199e-02,\n",
       "          -1.2975e-01, -2.7307e-02,  1.2558e-02, -5.2604e-02,  2.1909e-02,\n",
       "          -1.1033e-01, -6.1073e-02, -2.5711e-02, -2.8864e-02,  1.1442e-01,\n",
       "           1.0797e-01, -2.7385e-02,  1.0366e-01,  5.7208e-02, -3.1134e-02,\n",
       "          -1.1716e-01, -1.0339e-01,  3.0284e-02, -2.3341e-02,  4.5882e-02,\n",
       "           1.0555e-01, -9.0722e-02, -3.2170e-03,  4.5985e-02,  3.4212e-03,\n",
       "          -7.1112e-02, -4.2142e-02, -1.3852e-01, -5.5649e-02, -4.1576e-02,\n",
       "           9.0815e-02, -5.8173e-03,  1.7489e-02, -9.6912e-02, -1.4410e-01,\n",
       "          -7.1850e-03,  8.4507e-03, -4.5156e-02,  1.6635e-01,  8.1852e-02,\n",
       "          -8.1801e-02,  5.6258e-02,  3.4731e-02,  2.2535e-02,  9.3367e-03,\n",
       "          -4.1842e-02,  4.3779e-02,  1.0188e-01,  5.1298e-02,  4.5966e-02,\n",
       "          -7.1403e-02, -1.2346e-02, -7.7004e-02, -1.1398e-02, -5.8009e-02,\n",
       "           7.1091e-02, -9.9882e-02,  3.3283e-02, -1.3606e-02, -1.8915e-02,\n",
       "          -3.8656e-02, -1.0724e-01, -2.2161e-03, -2.2502e-02, -5.3425e-02,\n",
       "           3.7424e-02,  7.0944e-02, -4.7830e-02, -7.3630e-02,  6.1356e-02,\n",
       "          -1.2922e-01, -4.2971e-02,  4.7815e-02, -5.3503e-02,  2.0877e-02,\n",
       "           1.1314e-02, -5.0428e-02, -5.3935e-02,  4.3499e-02, -1.5027e-02,\n",
       "           1.7246e-02,  2.9226e-02,  4.0776e-02, -4.9548e-02,  6.0667e-02,\n",
       "          -1.2423e-01,  3.8583e-03, -1.5863e-02, -1.8801e-02, -1.6042e-02,\n",
       "          -3.6726e-02, -3.5068e-02, -1.7217e-02,  3.2096e-02,  1.5377e-01,\n",
       "          -1.7483e-01,  9.9309e-02,  8.9861e-02, -5.5493e-02,  6.4201e-03,\n",
       "           4.6726e-02, -6.4665e-02,  8.7811e-02, -2.0583e-04, -6.9936e-02,\n",
       "          -3.8584e-02, -6.8664e-02, -8.8481e-02,  1.2140e-01,  1.7137e-02,\n",
       "          -1.2348e-01,  8.7181e-03,  7.9938e-02, -5.2340e-02, -4.4158e-03,\n",
       "           2.9749e-02,  6.7697e-02, -2.6525e-02, -1.5741e-02, -1.0122e-01,\n",
       "          -2.5024e-02,  4.0475e-03,  2.3375e-02,  7.8196e-04, -3.9764e-02,\n",
       "          -2.5558e-03, -3.1749e-02,  4.3294e-03, -4.0102e-02, -7.4914e-02,\n",
       "          -2.2913e-02, -6.8846e-03, -6.1746e-02, -6.0632e-02,  7.4015e-02,\n",
       "           1.3436e-02, -2.6405e-02,  8.4054e-02,  7.2614e-03, -7.4828e-02,\n",
       "          -3.7266e-02, -3.0665e-04,  1.1812e-01, -7.5046e-02,  4.0008e-02,\n",
       "           8.3744e-02,  1.7661e-02,  8.4375e-02, -3.1443e-04, -7.8628e-03,\n",
       "          -1.5688e-02,  8.5084e-02,  8.1721e-02,  5.2687e-02, -8.2874e-02,\n",
       "          -2.7423e-02, -3.4881e-02, -1.4164e-02,  1.2083e-02, -4.8566e-02,\n",
       "          -5.3566e-02, -5.4015e-02, -4.6456e-02, -8.0718e-02,  4.7345e-02,\n",
       "          -7.8496e-02,  6.9961e-02, -8.1030e-02,  3.1385e-02,  3.4209e-02,\n",
       "          -2.2604e-02, -3.8018e-02,  6.4631e-04,  1.3364e-01,  1.0855e-02,\n",
       "           5.1403e-03, -4.0756e-02,  1.6979e-02, -4.9302e-02, -1.4254e-04,\n",
       "           1.2092e-01, -1.9394e-02, -3.7652e-02,  6.2363e-02,  2.7248e-02,\n",
       "          -3.0494e-03,  6.2137e-02, -2.6717e-02, -1.0675e-01,  7.3711e-02,\n",
       "           1.4264e-01, -1.7223e-02, -5.1589e-04,  1.2843e-01, -2.6990e-02,\n",
       "           4.4719e-02,  1.2488e-02, -2.4640e-02, -1.1718e-01,  4.1061e-04,\n",
       "          -1.0052e-01, -2.1677e-02,  1.2263e-01, -1.5344e-01,  2.1389e-02,\n",
       "          -4.0085e-02, -3.0447e-03,  2.3672e-02, -5.1782e-02, -3.7225e-02,\n",
       "          -6.6202e-02,  2.0466e-02,  2.4230e-02,  4.4372e-02, -3.5209e-02,\n",
       "           1.2648e-01,  9.2018e-03,  2.1656e-02,  6.3070e-02,  3.9903e-02,\n",
       "          -5.4706e-02, -1.6950e-02,  6.0609e-02, -5.4088e-02,  5.6424e-02,\n",
       "           4.5581e-02,  1.5122e-01, -4.9301e-02,  2.9525e-02,  1.2086e-01,\n",
       "          -1.1906e-02,  7.1302e-03,  7.8443e-02,  4.1536e-02,  1.3911e-02,\n",
       "           5.6852e-02,  9.0302e-02,  5.6653e-02,  4.3161e-02, -3.8078e-02,\n",
       "           8.1836e-02,  8.5840e-02,  5.3626e-02, -1.3545e-02, -5.4254e-02,\n",
       "          -3.8996e-02, -1.3392e-01, -6.4284e-02,  2.4332e-02, -9.4033e-03,\n",
       "          -6.4130e-02, -1.3948e-02,  4.1151e-02,  4.4773e-02,  5.6046e-02,\n",
       "          -1.5226e-01, -2.0774e-01,  3.9828e-02,  7.2552e-02,  6.1654e-02,\n",
       "          -2.3129e-02,  5.8056e-02,  5.1142e-02,  5.5600e-02, -1.3817e-01,\n",
       "           1.4418e-01,  1.2543e-01,  5.3810e-02,  1.4356e-02, -6.6895e-02,\n",
       "          -5.2549e-02, -6.5879e-02,  7.9847e-02,  1.7307e-02, -3.9963e-03,\n",
       "           4.6567e-02, -4.7679e-02,  8.5466e-02, -8.0795e-02,  3.3541e-02,\n",
       "          -8.6972e-02, -7.7829e-02,  1.2421e-04,  2.4862e-02, -7.9958e-02,\n",
       "          -6.7859e-02, -1.8292e-01,  2.3604e-02,  1.6810e-01,  3.0391e-02,\n",
       "          -1.0909e-01,  9.7894e-02,  5.5173e-02, -5.0691e-02,  6.9286e-02,\n",
       "          -4.9996e-02,  3.5478e-02, -5.0023e-02,  4.6548e-02, -5.8731e-02,\n",
       "          -5.2153e-02,  6.1406e-02, -5.0932e-02, -4.4377e-02,  4.1101e-02,\n",
       "          -2.6994e-02,  1.3397e-01,  3.7453e-02,  9.4914e-03, -6.7886e-02,\n",
       "           4.5710e-02, -1.0628e-01,  2.1610e-02, -4.6527e-02, -1.2443e-01,\n",
       "          -6.3709e-02,  2.9495e-02,  7.0215e-02, -5.2558e-02, -1.9932e-02,\n",
       "           3.8935e-02,  3.2594e-03,  3.4255e-02, -1.7333e-02,  6.3880e-03,\n",
       "           1.5504e-02,  1.3890e-01,  1.8180e-02, -1.8795e-02, -9.5176e-02,\n",
       "           3.4061e-02,  2.3585e-02,  5.3090e-02,  3.4990e-02,  2.4611e-02,\n",
       "          -1.9052e-01, -3.6740e-02,  5.8895e-02,  1.2622e-01,  5.3445e-02,\n",
       "          -4.6055e-04,  2.6665e-02,  9.5453e-02,  5.2916e-02, -7.9737e-02,\n",
       "           2.7248e-02, -3.8851e-03, -2.2787e-02, -7.3972e-02,  9.1083e-03,\n",
       "           8.1910e-02, -7.9965e-02, -2.5057e-02,  1.2993e-02, -2.3432e-02,\n",
       "          -1.2953e-01,  7.9266e-02, -4.8694e-02, -7.6346e-03,  1.2763e-01,\n",
       "           5.2873e-02, -1.1085e-01,  5.9998e-03, -2.7584e-02, -1.2101e-01,\n",
       "          -1.1134e-01, -6.7186e-02, -1.1606e-02,  3.9481e-02,  5.6450e-02,\n",
       "           4.2094e-02,  2.4733e-03,  1.2158e-01,  5.8778e-02, -5.1661e-02,\n",
       "          -2.2007e-02, -6.7656e-02, -7.2615e-02, -1.4584e-02, -2.3516e-02,\n",
       "          -6.3404e-02, -8.1590e-03,  6.7420e-02,  1.7352e-03,  6.2614e-02,\n",
       "          -7.8014e-02,  2.1896e-02, -5.9308e-02, -8.5715e-02,  2.5642e-02,\n",
       "           2.9217e-02, -3.8312e-02,  3.4352e-02, -1.0466e-01,  1.1710e-02,\n",
       "           6.2727e-02, -1.1218e-02, -4.3198e-02,  3.5521e-02,  8.8414e-02,\n",
       "           4.6967e-02, -6.3616e-02,  2.1561e-02, -5.3509e-02,  1.5474e-02,\n",
       "          -1.9632e-04, -5.4989e-02,  6.4157e-02,  3.7885e-02, -9.1769e-02,\n",
       "           1.2104e-01,  2.8349e-02,  3.0541e-02, -1.5458e-02,  1.3772e-03,\n",
       "           2.1433e-02, -4.7397e-02, -8.4097e-03, -4.3591e-02,  7.7351e-03,\n",
       "           6.5787e-03, -3.5452e-02,  1.2631e-01, -9.7942e-03, -7.0819e-02,\n",
       "          -1.2372e-02,  7.1967e-03, -9.9302e-02, -3.4474e-02, -9.0422e-02,\n",
       "           3.3433e-02, -1.3463e-02, -7.0926e-02,  2.1002e-02, -5.7188e-02,\n",
       "          -3.2017e-02, -6.6981e-02,  2.2548e-02, -7.8253e-02, -5.4752e-02,\n",
       "           1.6316e-02, -2.8965e-02,  1.9766e-02,  7.6046e-02, -1.1449e-03,\n",
       "          -2.6520e-03, -8.1509e-02, -2.8717e-02, -6.3732e-02,  3.8445e-02,\n",
       "          -7.8189e-02, -1.1902e-01, -9.1320e-02,  1.7014e-02,  1.7339e-02,\n",
       "          -1.0909e-02, -2.6038e-03, -5.6788e-02, -3.4965e-02,  1.9869e-02,\n",
       "           5.1414e-02, -5.9255e-02,  5.6688e-02, -7.9898e-04,  4.2182e-02,\n",
       "          -2.9043e-02,  5.5778e-02,  5.5455e-03]], device='cuda:0',\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_vector, predict_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.choice(range(0, len(sentences)))\n",
    "masked_words[idx], sentences[idx], predicted_sentences[idx], softmax_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_df = []\n",
    "for (model_language, dataset_language), score in results.items():\n",
    "    to_df.append({\n",
    "        \"model_language\" : model_language,\n",
    "        \"dataset_language\" : dataset_language,\n",
    "        \"avg_score\" : score[0],\n",
    "        \"median_score\" : score[1],\n",
    "        \"cos_simil\" : score[2]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "bunch = GGBunch()\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='cos_dist '), data=df.query(\"model_language != 'es'\")) + scale_fill_gradient(low='white', high='blue') + ggsize(500, 500) + ggtitle(\"Cosine Similarity\")\n",
    ", 0, 0)\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='avg_score'), data=df.query(\"model_language != 'es'\")) + scale_fill_gradient(low='white', high='red') + ggsize(500, 500) + ggtitle(\"Average Score\")\n",
    ", 500, 0)\n",
    "\n",
    "bunch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggsave(bunch, \"results.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32717386921c27a635de762f5210753becc6f37ba8fb6494d8fab72c3a611423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
