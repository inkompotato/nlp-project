{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inkompotato/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, RobertaForQuestionAnswering, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    name: str\n",
    "    tokenizer: object\n",
    "    model: object\n",
    "\n",
    "    def __init__(self, name: str, tokenizer: object = None, model: object = None):\n",
    "        self.name = name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Model(name={self.name})\"\n",
    "\n",
    "    def set_tokenizer(self, tokenizer: object):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_model(self, model: object):\n",
    "        self.model = model\n",
    "\n",
    "    def to_pipeline(self):\n",
    "        return pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0)\n",
    "\n",
    "    def get_collator(self):\n",
    "        return DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 204kB/s]\n",
      "Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.76MB/s]\n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.09MB/s]\n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.61MB/s]\n",
      "Downloading: 100%|██████████| 478M/478M [00:06<00:00, 72.7MB/s]\n",
      "Downloading: 100%|██████████| 1.09k/1.09k [00:00<00:00, 656kB/s]\n",
      "Downloading: 100%|██████████| 831k/831k [00:00<00:00, 1.98MB/s]\n",
      "Downloading: 100%|██████████| 497k/497k [00:00<00:00, 1.20MB/s]\n",
      "Downloading: 100%|██████████| 2.11M/2.11M [00:00<00:00, 3.72MB/s]\n",
      "Downloading: 100%|██████████| 772/772 [00:00<00:00, 407kB/s]\n",
      "Downloading: 100%|██████████| 674/674 [00:00<00:00, 320kB/s]\n",
      "Downloading: 100%|██████████| 476M/476M [00:04<00:00, 112MB/s]\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 35.1kB/s]\n",
      "Downloading: 100%|██████████| 385/385 [00:00<00:00, 127kB/s]\n",
      "Downloading: 100%|██████████| 245k/245k [00:00<00:00, 747kB/s]\n",
      "Downloading: 100%|██████████| 424M/424M [00:04<00:00, 109MB/s]\n",
      "Some weights of the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 5/5 [00:49<00:00,  9.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "models = {\n",
    "    \"de\" : Model(\"uklfr/gottbert-base\"),\n",
    "    \"nl\" : Model(\"pdelobelle/robbert-v2-dutch-base\"),\n",
    "    \"en\" : Model(\"bert-base\"),\n",
    "    \"es\" : Model(\"bertin-project/bertin-roberta-base-spanish\"),\n",
    "    \"tr\" : Model(\"dbmdz/bert-base-turkish-cased\")\n",
    "}\n",
    "\n",
    "for language, model in tqdm(models.items()):\n",
    "    # get model & tokenizer from huggingface\n",
    "    model.set_tokenizer(AutoTokenizer.from_pretrained(model.name))\n",
    "    model.set_model(AutoModelForMaskedLM.from_pretrained(model.name).to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines\n",
    "\n",
    "pipelines = {k: v.to_pipeline() for k, v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load datasets\n",
    "\n",
    "datasets = {\n",
    "    \"de\" : load_dataset(\"xquad\", \"xquad.de\"),\n",
    "    \"es\" : load_dataset(\"xquad\", \"xquad.es\"),\n",
    "    \"en\" : load_dataset(\"xquad\", \"xquad.en\"),\n",
    "    \"tr\" : load_dataset(\"xquad\", \"xquad.tr\"),\n",
    "    \"ro\" : load_dataset(\"xquad\", \"xquad.ro\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - de: 0.46071538940542683\n",
      "de - es: 0.3567187782143202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inkompotato/miniconda3/envs/nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:996: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - en: 0.39938523550053423\n",
      "de - tr: 0.28283499372129534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [04:25<17:42, 265.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - ro: 0.3047445067640461\n",
      "nl - de: 0.25299656951255545\n",
      "nl - es: 0.2479078366528343\n",
      "nl - en: 0.3053643304479345\n",
      "nl - tr: 0.20374616370099918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [08:39<12:55, 258.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - ro: 0.19068813664510717\n",
      "en - de: 0.5488932316252907\n",
      "en - es: 0.609008788265456\n",
      "en - en: 0.6592554890597058\n",
      "en - tr: 0.4299370428742694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [13:02<08:41, 260.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en - ro: 0.5608771198478245\n",
      "es - de: 0.26342204932808266\n",
      "es - es: 0.48206794604227443\n",
      "es - en: 0.35937982430666865\n",
      "es - tr: 0.21336857984569754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [17:29<04:23, 263.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es - ro: 0.21655181313657607\n",
      "tr - de: 0.3109657022356408\n",
      "tr - es: 0.31926372437494843\n",
      "tr - en: 0.37528451141533153\n",
      "tr - tr: 0.4052198801610779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [21:24<00:00, 256.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr - ro: 0.2733037169898011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# tokenize each dataset for each model\n",
    "for model_language, pipeline in tqdm(pipelines.items()):\n",
    "\n",
    "    for dataset_language, dataset in datasets.items():\n",
    "        # raw data\n",
    "        raw_data = dataset[\"validation\"][\"context\"]\n",
    "        # split into sentences\n",
    "        # TODO: split at all sentence separators (not just .)\n",
    "        sentences = [sentence.split(\" \") for text in raw_data for sentence in text.split(\".\")]       \n",
    "        # mask one word in each sentence\n",
    "        for sentence in sentences:\n",
    "            # get random index\n",
    "            index = random.randint(0, len(sentence) - 1)\n",
    "            # mask word\n",
    "            sentence[index] = pipeline.tokenizer.mask_token\n",
    "\n",
    "        sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline(sentences)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error with {model_language} and {dataset_language}\")\n",
    "            continue\n",
    "\n",
    "        # get average score\n",
    "        scores = [prediction[0][\"score\"] for prediction in predictions]\n",
    "        average_score = np.mean(scores)\n",
    "        print(f\"{model_language} - {dataset_language}: {average_score}\")\n",
    "        results[(model_language, dataset_language)] = average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_df = []\n",
    "for (model_language, dataset_language), score in results.items():\n",
    "    to_df.append({\n",
    "        \"model_language\" : model_language,\n",
    "        \"dataset_language\" : dataset_language,\n",
    "        \"score\" : score\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_language</th>\n",
       "      <th>dataset_language</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>0.460715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de</td>\n",
       "      <td>es</td>\n",
       "      <td>0.356719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "      <td>0.399385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>tr</td>\n",
       "      <td>0.282835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de</td>\n",
       "      <td>ro</td>\n",
       "      <td>0.304745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_language dataset_language     score\n",
       "0             de               de  0.460715\n",
       "1             de               es  0.356719\n",
       "2             de               en  0.399385\n",
       "3             de               tr  0.282835\n",
       "4             de               ro  0.304745"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"VQr2pk\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.5.1/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"VQr2pk\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"VQr2pk\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"JtVop0\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500,\n",
       "\"height\":500\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"blue\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"data\":{\n",
       "\"model_language\":[\"de\",\"de\",\"de\",\"de\",\"de\",\"nl\",\"nl\",\"nl\",\"nl\",\"nl\",\"en\",\"en\",\"en\",\"en\",\"en\",\"es\",\"es\",\"es\",\"es\",\"es\",\"tr\",\"tr\",\"tr\",\"tr\",\"tr\"],\n",
       "\"dataset_language\":[\"de\",\"es\",\"en\",\"tr\",\"ro\",\"de\",\"es\",\"en\",\"tr\",\"ro\",\"de\",\"es\",\"en\",\"tr\",\"ro\",\"de\",\"es\",\"en\",\"tr\",\"ro\",\"de\",\"es\",\"en\",\"tr\",\"ro\"],\n",
       "\"score\":[0.46071538940542683,0.3567187782143202,0.39938523550053423,0.28283499372129534,0.3047445067640461,0.25299656951255545,0.2479078366528343,0.3053643304479345,0.20374616370099918,0.19068813664510717,0.5488932316252907,0.609008788265456,0.6592554890597058,0.4299370428742694,0.5608771198478245,0.26342204932808266,0.48206794604227443,0.35937982430666865,0.21336857984569754,0.21655181313657607,0.3109657022356408,0.31926372437494843,0.37528451141533153,0.4052198801610779,0.2733037169898011]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"model_language\",\n",
       "\"y\":\"dataset_language\",\n",
       "\"fill\":\"score\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"JtVop0\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ],
      "text/plain": [
       "<lets_plot.plot.core.PlotSpec at 0x7f734beaace0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='score'), data=df) + scale_fill_gradient(low='white', high='blue') + ggsize(500, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32717386921c27a635de762f5210753becc6f37ba8fb6494d8fab72c3a611423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
