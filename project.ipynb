{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inkompotato/miniconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, RobertaForQuestionAnswering, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    name: str\n",
    "    tokenizer: object\n",
    "    model: object\n",
    "\n",
    "    def __init__(self, name: str, tokenizer: object = None, model: object = None):\n",
    "        self.name = name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Model(name={self.name})\"\n",
    "\n",
    "    def set_tokenizer(self, tokenizer: object):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_model(self, model: object):\n",
    "        self.model = model\n",
    "\n",
    "    def to_pipeline(self):\n",
    "        return pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0, top_k = 1)\n",
    "\n",
    "    def get_collator(self):\n",
    "        return DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "models = {\n",
    "    \"de\" : Model(\"uklfr/gottbert-base\"),\n",
    "    # \"nl\" : Model(\"pdelobelle/robbert-v2-dutch-base\"),\n",
    "    # \"es\" : Model(\"bertin-project/bertin-roberta-base-spanish\"),\n",
    "    # \"tr\" : Model(\"dbmdz/bert-base-turkish-cased\")\n",
    "}\n",
    "\n",
    "for language, model in tqdm(models.items()):\n",
    "    # get model & tokenizer from huggingface\n",
    "    model.set_tokenizer(AutoTokenizer.from_pretrained(model.name))\n",
    "    model.set_model(AutoModelForMaskedLM.from_pretrained(model.name).to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines\n",
    "\n",
    "pipelines = {k: v.to_pipeline() for k, v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load datasets\n",
    "\n",
    "datasets = {\n",
    "    \"de\" : load_dataset(\"xquad\", \"xquad.de\"),\n",
    "    \"es\" : load_dataset(\"xquad\", \"xquad.es\"),\n",
    "    # \"tr\" : load_dataset(\"xquad\", \"xquad.tr\"),\n",
    "    # \"ro\" : load_dataset(\"xquad\", \"xquad.ro\"),\n",
    "    \"en\" : load_dataset(\"xquad\", \"xquad.en\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - de: 0.4654309966866726, 0.16268255323756378\n",
      "de - es: 0.3568663840078348, 0.08487572332219087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:29<00:00, 149.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - en: 0.3958673913783324, 0.09840186539969191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# tokenize each dataset for each model\n",
    "for model_language, pipeline in tqdm(pipelines.items()):\n",
    "\n",
    "    for dataset_language, dataset in datasets.items():\n",
    "        # raw data\n",
    "        raw_data = dataset[\"validation\"][\"context\"]\n",
    "        # split into sentences\n",
    "        # TODO: permorm additional data cleaning\n",
    "        sentences = [sentence.split(\" \") for text in raw_data for sentence in text.split(\".\")]\n",
    "        masked_words = []     \n",
    "        # mask one word in each sentence\n",
    "        for sentence in sentences:\n",
    "            # get random index\n",
    "            index = random.randint(0, len(sentence) - 1)\n",
    "            # mask word\n",
    "            masked_words.append(sentence[index])\n",
    "            sentence[index] = pipeline.tokenizer.mask_token\n",
    "\n",
    "        sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline(sentences)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error with {model_language} and {dataset_language}\")\n",
    "            continue\n",
    "\n",
    "        # get average score\n",
    "        softmax_scores = [prediction[0][\"score\"] for prediction in predictions]\n",
    "        predicted_sentences = [prediction[0][\"sequence\"] for prediction in predictions]\n",
    "        prediced_words = [prediction[0][\"token_str\"] for prediction in predictions]\n",
    "        # get vector for masked and predicted word\n",
    "        masked_index = [pipeline.tokenizer.encode(token, add_special_tokens=False) for token in masked_words]\n",
    "        predict_index = [pipeline.tokenizer.encode(token, add_special_tokens=False) for token in prediced_words]\n",
    "        # calculate cosine similarity between predicted words and masked words\n",
    "        cosine_similarities = []\n",
    "        for i in range(len(masked_index)):\n",
    "            if masked_index[i] == [] or predict_index[i] == []:\n",
    "                cosine_similarities.append(np.array([0]))\n",
    "                continue\n",
    "            # get vector for masked word\n",
    "            masked_vector = pipeline.model.roberta.embeddings.word_embeddings(torch.tensor(masked_index[i]).to(\"cuda\"))\n",
    "            # get vector for predicted word\n",
    "            predict_vector = pipeline.model.roberta.embeddings.word_embeddings(torch.tensor(predict_index[i]).to(\"cuda\"))\n",
    "\n",
    "            # calculate cosine similarity using torch\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(masked_vector, predict_vector, dim=1).cpu().detach().numpy()\n",
    "            cosine_similarities.append(cos_sim)\n",
    "\n",
    "        # mean cosine similarity\n",
    "        # flatten cos sim\n",
    "        cosine_similarities = [abs(item) for sublist in cosine_similarities for item in sublist]\n",
    "        mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "        average_score = np.mean(softmax_scores)\n",
    "        median_score = np.median(softmax_scores)\n",
    "        print(f\"{model_language} - {dataset_language}: {average_score}, {mean_cosine_similarity}\")\n",
    "        results[(model_language, dataset_language)] = [average_score, median_score, mean_cosine_similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.choice(range(0, len(sentences)))\n",
    "masked_words[idx], sentences[idx], predicted_sentences[idx], softmax_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_df = []\n",
    "for (model_language, dataset_language), score in results.items():\n",
    "    to_df.append({\n",
    "        \"model_language\" : model_language,\n",
    "        \"dataset_language\" : dataset_language,\n",
    "        \"avg_score\" : score[0],\n",
    "        \"median_score\" : score[1],\n",
    "        \"perplexity\" : score[2]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='perplexity'), data=df) + scale_fill_gradient(low='orange', high='blue') + ggsize(500, 500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32717386921c27a635de762f5210753becc6f37ba8fb6494d8fab72c3a611423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
