{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, RobertaForQuestionAnswering, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    name: str\n",
    "    tokenizer: object\n",
    "    model: object\n",
    "\n",
    "    def __init__(self, name: str, tokenizer: object = None, model: object = None):\n",
    "        self.name = name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Model(name={self.name})\"\n",
    "\n",
    "    def set_tokenizer(self, tokenizer: object):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_model(self, model: object):\n",
    "        self.model = model\n",
    "\n",
    "    def to_pipeline(self):\n",
    "        return pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0, top_k = 1)\n",
    "\n",
    "    def get_collator(self):\n",
    "        return DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "\tdef __init__(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number =0\n",
    "\t\tself.str = 'perplexity'\n",
    "\t\tself.metric_scores = {}\n",
    "\n",
    "\t# cross entropy loss\n",
    "\tdef score(self, loss):\n",
    "\t\tself.sum += loss\n",
    "\t\t# should be number of sentences\n",
    "\t\tself.number += 1\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number = 0\n",
    "\n",
    "\tdef get_score(self):\n",
    "\t\tif self.sum ==0:\n",
    "\t\t\tself.metric_scores[self.str] = 0.0\n",
    "\t\tself.metric_scores[self.str] = math.exp(self.sum / self.number)\n",
    "\t\tself.metric_scores[\"sum\"] = self.str\n",
    "\t\treturn self.metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:19<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "models = {\n",
    "    \"de\" : Model(\"uklfr/gottbert-base\"),\n",
    "    \"nl\" : Model(\"pdelobelle/robbert-v2-dutch-base\"),\n",
    "    \"se\" : Model(\"birgermoell/roberta-swedish\"),\n",
    "    \"dk\" : Model(\"DDSC/roberta-base-danish\"),\n",
    "}\n",
    "\n",
    "for language, model in tqdm(models.items()):\n",
    "    # get model & tokenizer from huggingface\n",
    "    model.set_tokenizer(AutoTokenizer.from_pretrained(model.name, model_max_length=512))\n",
    "    model.set_model(AutoModelForMaskedLM.from_pretrained(model.name).to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines\n",
    "\n",
    "pipelines = {k: v.to_pipeline() for k, v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load datasets\n",
    "\n",
    "def load_dataset_local(dataset_name):\n",
    "    with open(f\"wiki_data/{dataset_name}.txt\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "datasets = {\n",
    "    \"nl\" : load_dataset_local(\"nlwiki_sentences\"),\n",
    "    \"dk\" : load_dataset_local(\"dawiki_sentences\"),\n",
    "    \"se\" : load_dataset_local(\"svwiki_sentences\"),\n",
    "    \"de\" : load_dataset_local(\"dewiki_sentences\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:05<00:00, 133.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - nl: 0.28799366936014364, 0.49485372149553464, 131126265.43179055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:05<00:00, 139.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - dk: 0.25232107056116804, 0.24082141665132953, 2898.096486008697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 743/743 [00:07<00:00, 100.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - se: 0.23813699464233723, 0.128770391361013, 4200610.145795178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 694/694 [00:02<00:00, 257.51it/s]\n",
      " 25%|██▌       | 1/4 [05:12<15:37, 312.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de - de: 0.49317091358430865, 0.5455624932683708, 89853317078.90202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:02<00:00, 267.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - nl: 0.5185370977312649, 0.5437241566452584, 398850193.39156175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:04<00:00, 147.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - dk: 0.17897887501138462, 0.23401108517735555, 14397.090109044244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 743/743 [00:04<00:00, 182.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - se: 0.18017199604387116, 0.10809311741740346, 26868733.755516145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 694/694 [00:05<00:00, 138.39it/s]\n",
      " 50%|█████     | 2/4 [10:05<10:02, 301.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl - de: 0.2577283655412955, 0.4970116746180915, 4037656924.3197265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:04<00:00, 154.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - nl: 0.25600145236898453, 0.493979878364008, 111043148.30228892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:03<00:00, 182.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - dk: 0.31182136101284147, 0.27232591309601284, 450140.72381914913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 743/743 [00:02<00:00, 258.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - se: 0.42099222592509317, 0.23732886433232087, 226911764.1768161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 694/694 [00:04<00:00, 146.61it/s]\n",
      " 75%|███████▌  | 3/4 [15:01<04:58, 298.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se - de: 0.2643633002816717, 0.4949022586074477, 2916742796.5731316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 697/697 [00:04<00:00, 148.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dk - nl: 0.2923883924571594, 0.4865246465280186, 99989013.2068993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 721/721 [00:02<00:00, 257.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dk - dk: 0.4362164365570731, 0.3069264163017941, 185597.7234408888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 743/743 [00:05<00:00, 139.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dk - se: 0.29689900109596135, 0.20132672174656221, 84420212.9207603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "100%|██████████| 694/694 [00:04<00:00, 146.85it/s]\n",
      "100%|██████████| 4/4 [19:53<00:00, 298.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dk - de: 0.27977260160608414, 0.4847021962209376, 2065837084.773247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# tokenize each dataset for each model\n",
    "for model_language, pipeline in tqdm(pipelines.items()):\n",
    "\n",
    "    for dataset_language, dataset in datasets.items():\n",
    "        # raw data\n",
    "        raw_data = dataset\n",
    "        # split into sentences\n",
    "        # TODO: permorm additional data cleaning\n",
    "        sentences = [sentence.split(\" \") for text in raw_data for sentence in text.split(\".\")]\n",
    "        masked_words = []     \n",
    "        # mask one word in each sentence\n",
    "        for sentence in sentences:\n",
    "            # get random index\n",
    "            index = random.randint(0, len(sentence) - 1)\n",
    "            # mask word\n",
    "            masked_words.append(sentence[index])\n",
    "            sentence[index] = pipeline.tokenizer.mask_token\n",
    "\n",
    "        sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline(sentences)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error with {model_language} and {dataset_language}\")\n",
    "            continue\n",
    "\n",
    "        # get average score\n",
    "        softmax_scores = [prediction[0][\"score\"] for prediction in predictions]\n",
    "        predicted_sentences = [prediction[0][\"sequence\"] for prediction in predictions]\n",
    "        predicted_words = [prediction[0][\"token_str\"] for prediction in predictions]\n",
    "        # get vector for masked and predicted word\n",
    "        masked_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=True) for token in masked_words]\n",
    "        predict_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=True) for token in predicted_words]\n",
    "        unknown_index = pipelines[dataset_language].tokenizer.encode(pipelines[dataset_language].tokenizer.unk_token, add_special_tokens=True)\n",
    "        word_pairs[f\"{model_language}-{dataset_language}\"] = [masked_words, predicted_words]\n",
    "        nr_unks = []\n",
    "        ul = len(unknown_index)\n",
    "        for i in range(len(masked_index)):\n",
    "            ml = len(masked_index[i])\n",
    "            pl = len(predict_index[i])\n",
    "            diff = abs(ml - pl)\n",
    "            repeats = diff // ul\n",
    "            nr_unks.append(repeats + 1)\n",
    "            if ml < pl and diff % ul == 0:\n",
    "                # diff is a multiple of ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index * repeats\n",
    "            elif ml < pl and diff % ul != 0 and diff < ul:\n",
    "                # diff is smaller than ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index[:diff]\n",
    "            elif ml < pl and diff % ul != 0 and diff >= ul:\n",
    "                # diff is larger than ul and not a multiple of ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index * repeats + unknown_index[:diff % ul]\n",
    "            elif ml > pl and diff % ul == 0:\n",
    "                predict_index[i] = predict_index[i] + unknown_index * repeats\n",
    "            elif ml > pl and diff % ul != 0 and diff < ul:\n",
    "                predict_index[i] = predict_index[i] + unknown_index[:diff]\n",
    "            elif ml > pl and diff % ul != 0 and diff >= ul:\n",
    "                predict_index[i] = predict_index[i] + unknown_index * repeats + unknown_index[:diff % ul]\n",
    "\n",
    "        # calculate cosine similarity between predicted words and masked words\n",
    "        cosine_similarities = []\n",
    "        for i in range(len(masked_index)):\n",
    "            if masked_index[i] == [] or predict_index[i] == []:\n",
    "                cosine_similarities.append(np.array(0))\n",
    "                continue\n",
    "            # get vector for masked word\n",
    "            masked_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(masked_index[i]).to(\"cuda\"))\n",
    "            # get vector for predicted word\n",
    "            predict_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(predict_index[i]).to(\"cuda\"))\n",
    "            # if masked_vector.shape[0] < predict_vector.shape[0]:\n",
    "            #     # extend with unk_vector until shape is the same\n",
    "            #     masked_vector = torch.nn.functional.pad(masked_vector, (0, 0, 0, predict_vector.shape[0] - masked_vector.shape[0]))\n",
    "            # elif masked_vector.shape[0] > predict_vector.shape[0]:\n",
    "            #     predict_vector = torch.nn.functional.pad(predict_vector, (0, 0, 0, masked_vector.shape[0] - predict_vector.shape[0]))\n",
    "\n",
    "            # calculate cosine similarity using torch\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(masked_vector, predict_vector, dim=1).cpu().detach().numpy()\n",
    "            cosine_similarities.append(np.mean(cos_sim) / nr_unks[i])\n",
    "\n",
    "        # calculate perplexity\n",
    "        perplexity_scores = perplexity.compute(predictions=predicted_words, references=masked_words, model_id=models[dataset_language].name)\n",
    "        perplexity_score = np.mean(perplexity_scores[\"perplexities\"]) / len(masked_words)\n",
    "        # mean cosine similarity\n",
    "        # flatten cos sim\n",
    "        # cosine_similarities = [item for sublist in cosine_similarities for item in sublist]\n",
    "        mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "        average_score = np.mean(softmax_scores)\n",
    "        median_score = np.median(softmax_scores)\n",
    "        print(f\"{model_language} - {dataset_language}: {average_score}, {mean_cosine_similarity}, {perplexity_score}\")\n",
    "        results[(model_language, dataset_language)] = [average_score, median_score, mean_cosine_similarity, perplexity_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Molnár,',\n",
       " 'Nino Albarosa, Nino Antonellini, József <mask> Elsa Respighi, Nino Rota, Monika Ryba und Carlo Zecchi',\n",
       " 'Nino Albarosa, Nino Antonellini, József, Elsa Respighi, Nino Rota, Monika Ryba und Carlo Zecchi',\n",
       " 0.7408052086830139)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = random.choice(range(0, len(sentences)))\n",
    "masked_words[idx], sentences[idx], predicted_sentences[idx], softmax_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(word_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_df = []\n",
    "for (model_language, dataset_language), score in results.items():\n",
    "    to_df.append({\n",
    "        \"model_language\" : model_language,\n",
    "        \"dataset_language\" : dataset_language,\n",
    "        \"avg_score\" : score[0],\n",
    "        \"median_score\" : score[1],\n",
    "        \"cos_simil\" : score[2],\n",
    "        \"perplexity\" : score[3]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ranking\n",
    "df[\"rank\"] = df.groupby(\"dataset_language\")[\"cos_simil\"].rank(ascending=True)\n",
    "df.sort_values(by=[\"dataset_language\", \"model_language\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_language</th>\n",
       "      <th>dataset_language</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>median_score</th>\n",
       "      <th>cos_simil</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de</td>\n",
       "      <td>de</td>\n",
       "      <td>0.493171</td>\n",
       "      <td>0.424997</td>\n",
       "      <td>0.545562</td>\n",
       "      <td>8.985332e+10</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dk</td>\n",
       "      <td>de</td>\n",
       "      <td>0.279773</td>\n",
       "      <td>0.218365</td>\n",
       "      <td>0.484702</td>\n",
       "      <td>2.065837e+09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nl</td>\n",
       "      <td>de</td>\n",
       "      <td>0.257728</td>\n",
       "      <td>0.177668</td>\n",
       "      <td>0.497012</td>\n",
       "      <td>4.037657e+09</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>se</td>\n",
       "      <td>de</td>\n",
       "      <td>0.264363</td>\n",
       "      <td>0.200517</td>\n",
       "      <td>0.494902</td>\n",
       "      <td>2.916743e+09</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>de</td>\n",
       "      <td>dk</td>\n",
       "      <td>0.252321</td>\n",
       "      <td>0.186145</td>\n",
       "      <td>0.240821</td>\n",
       "      <td>2.898096e+03</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dk</td>\n",
       "      <td>dk</td>\n",
       "      <td>0.436216</td>\n",
       "      <td>0.363415</td>\n",
       "      <td>0.306926</td>\n",
       "      <td>1.855977e+05</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nl</td>\n",
       "      <td>dk</td>\n",
       "      <td>0.178979</td>\n",
       "      <td>0.111888</td>\n",
       "      <td>0.234011</td>\n",
       "      <td>1.439709e+04</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>se</td>\n",
       "      <td>dk</td>\n",
       "      <td>0.311821</td>\n",
       "      <td>0.230629</td>\n",
       "      <td>0.272326</td>\n",
       "      <td>4.501407e+05</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.287994</td>\n",
       "      <td>0.202455</td>\n",
       "      <td>0.494854</td>\n",
       "      <td>1.311263e+08</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dk</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.292388</td>\n",
       "      <td>0.221076</td>\n",
       "      <td>0.486525</td>\n",
       "      <td>9.998901e+07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nl</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.518537</td>\n",
       "      <td>0.487202</td>\n",
       "      <td>0.543724</td>\n",
       "      <td>3.988502e+08</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>se</td>\n",
       "      <td>nl</td>\n",
       "      <td>0.256001</td>\n",
       "      <td>0.182609</td>\n",
       "      <td>0.493980</td>\n",
       "      <td>1.110431e+08</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>de</td>\n",
       "      <td>se</td>\n",
       "      <td>0.238137</td>\n",
       "      <td>0.188770</td>\n",
       "      <td>0.128770</td>\n",
       "      <td>4.200610e+06</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dk</td>\n",
       "      <td>se</td>\n",
       "      <td>0.296899</td>\n",
       "      <td>0.190126</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>8.442021e+07</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nl</td>\n",
       "      <td>se</td>\n",
       "      <td>0.180172</td>\n",
       "      <td>0.131120</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>2.686873e+07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>se</td>\n",
       "      <td>se</td>\n",
       "      <td>0.420992</td>\n",
       "      <td>0.330078</td>\n",
       "      <td>0.237329</td>\n",
       "      <td>2.269118e+08</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_language dataset_language  avg_score  median_score  cos_simil  \\\n",
       "3              de               de   0.493171      0.424997   0.545562   \n",
       "15             dk               de   0.279773      0.218365   0.484702   \n",
       "7              nl               de   0.257728      0.177668   0.497012   \n",
       "11             se               de   0.264363      0.200517   0.494902   \n",
       "1              de               dk   0.252321      0.186145   0.240821   \n",
       "13             dk               dk   0.436216      0.363415   0.306926   \n",
       "5              nl               dk   0.178979      0.111888   0.234011   \n",
       "9              se               dk   0.311821      0.230629   0.272326   \n",
       "0              de               nl   0.287994      0.202455   0.494854   \n",
       "12             dk               nl   0.292388      0.221076   0.486525   \n",
       "4              nl               nl   0.518537      0.487202   0.543724   \n",
       "8              se               nl   0.256001      0.182609   0.493980   \n",
       "2              de               se   0.238137      0.188770   0.128770   \n",
       "14             dk               se   0.296899      0.190126   0.201327   \n",
       "6              nl               se   0.180172      0.131120   0.108093   \n",
       "10             se               se   0.420992      0.330078   0.237329   \n",
       "\n",
       "      perplexity  rank  \n",
       "3   8.985332e+10   4.0  \n",
       "15  2.065837e+09   1.0  \n",
       "7   4.037657e+09   3.0  \n",
       "11  2.916743e+09   2.0  \n",
       "1   2.898096e+03   2.0  \n",
       "13  1.855977e+05   4.0  \n",
       "5   1.439709e+04   1.0  \n",
       "9   4.501407e+05   3.0  \n",
       "0   1.311263e+08   3.0  \n",
       "12  9.998901e+07   1.0  \n",
       "4   3.988502e+08   4.0  \n",
       "8   1.110431e+08   2.0  \n",
       "2   4.200610e+06   2.0  \n",
       "14  8.442021e+07   3.0  \n",
       "6   2.686873e+07   1.0  \n",
       "10  2.269118e+08   4.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div id=\"Pcogxj\"></div>\n",
       "            <script type=\"text/javascript\" data-lets-plot-script=\"library\">\n",
       "                if(!window.letsPlotCallQueue) {\n",
       "                    window.letsPlotCallQueue = [];\n",
       "                }; \n",
       "                window.letsPlotCall = function(f) {\n",
       "                    window.letsPlotCallQueue.push(f);\n",
       "                };\n",
       "                (function() {\n",
       "                    var script = document.createElement(\"script\");\n",
       "                    script.type = \"text/javascript\";\n",
       "                    script.src = \"https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.5.1/js-package/distr/lets-plot.min.js\";\n",
       "                    script.onload = function() {\n",
       "                        window.letsPlotCall = function(f) {f();};\n",
       "                        window.letsPlotCallQueue.forEach(function(f) {f();});\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        \n",
       "                    };\n",
       "                    script.onerror = function(event) {\n",
       "                        window.letsPlotCall = function(f) {};    // noop\n",
       "                        window.letsPlotCallQueue = [];\n",
       "                        var div = document.createElement(\"div\");\n",
       "                        div.style.color = 'darkred';\n",
       "                        div.textContent = 'Error loading Lets-Plot JS';\n",
       "                        document.getElementById(\"Pcogxj\").appendChild(div);\n",
       "                    };\n",
       "                    var e = document.getElementById(\"Pcogxj\");\n",
       "                    e.appendChild(script);\n",
       "                })()\n",
       "            </script>\n",
       "            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "   <div id=\"44htXh\"></div>\n",
       "   <script type=\"text/javascript\" data-lets-plot-script=\"plot\">\n",
       "       (function() {\n",
       "           var plotSpec={\n",
       "\"kind\":\"ggbunch\",\n",
       "\"items\":[{\n",
       "\"x\":0,\n",
       "\"y\":0,\n",
       "\"width\":null,\n",
       "\"height\":null,\n",
       "\"feature_spec\":{\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500,\n",
       "\"height\":500\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Cosine Similarity\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"blue\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "},{\n",
       "\"aesthetic\":\"x\",\n",
       "\"discrete\":true,\n",
       "\"reverse\":true\n",
       "},{\n",
       "\"aesthetic\":\"y\",\n",
       "\"discrete\":true,\n",
       "\"reverse\":false\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"data\":{\n",
       "\"model_language\":[\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\"],\n",
       "\"dataset_language\":[\"de\",\"de\",\"de\",\"de\",\"dk\",\"dk\",\"dk\",\"dk\",\"nl\",\"nl\",\"nl\",\"nl\",\"se\",\"se\",\"se\",\"se\"],\n",
       "\"rank\":[4.0,1.0,3.0,2.0,2.0,4.0,1.0,3.0,3.0,1.0,4.0,2.0,2.0,3.0,1.0,4.0]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"model_language\",\n",
       "\"y\":\"dataset_language\",\n",
       "\"fill\":\"rank\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "}\n",
       "},{\n",
       "\"x\":500,\n",
       "\"y\":0,\n",
       "\"width\":null,\n",
       "\"height\":null,\n",
       "\"feature_spec\":{\n",
       "\"mapping\":{\n",
       "},\n",
       "\"data_meta\":{\n",
       "},\n",
       "\"ggsize\":{\n",
       "\"width\":500,\n",
       "\"height\":500\n",
       "},\n",
       "\"ggtitle\":{\n",
       "\"text\":\"Average Score\"\n",
       "},\n",
       "\"kind\":\"plot\",\n",
       "\"scales\":[{\n",
       "\"aesthetic\":\"fill\",\n",
       "\"low\":\"white\",\n",
       "\"high\":\"red\",\n",
       "\"scale_mapper_kind\":\"color_gradient\"\n",
       "},{\n",
       "\"aesthetic\":\"x\",\n",
       "\"discrete\":true,\n",
       "\"reverse\":true\n",
       "}],\n",
       "\"layers\":[{\n",
       "\"geom\":\"tile\",\n",
       "\"data\":{\n",
       "\"model_language\":[\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\",\"de\",\"dk\",\"nl\",\"se\"],\n",
       "\"dataset_language\":[\"de\",\"de\",\"de\",\"de\",\"dk\",\"dk\",\"dk\",\"dk\",\"nl\",\"nl\",\"nl\",\"nl\",\"se\",\"se\",\"se\",\"se\"],\n",
       "\"avg_score\":[0.49317091358430865,0.27977260160608414,0.2577283655412955,0.2643633002816717,0.25232107056116804,0.4362164365570731,0.17897887501138462,0.31182136101284147,0.28799366936014364,0.2923883924571594,0.5185370977312649,0.25600145236898453,0.23813699464233723,0.29689900109596135,0.18017199604387116,0.42099222592509317]\n",
       "},\n",
       "\"mapping\":{\n",
       "\"x\":\"model_language\",\n",
       "\"y\":\"dataset_language\",\n",
       "\"fill\":\"avg_score\"\n",
       "},\n",
       "\"data_meta\":{\n",
       "}\n",
       "}],\n",
       "\"metainfo_list\":[]\n",
       "}\n",
       "}]\n",
       "};\n",
       "           var plotContainer = document.getElementById(\"44htXh\");\n",
       "           window.letsPlotCall(function() {{\n",
       "               LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);\n",
       "           }});\n",
       "       })();    \n",
       "   </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "bunch = GGBunch()\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='rank'), data=df) + scale_fill_gradient(low='white', high='blue') + ggsize(500, 500) + ggtitle(\"Cosine Similarity\") + scale_x_discrete_reversed() + scale_y_discrete()\n",
    ", 0, 0)\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='avg_score'), data=df) + scale_fill_gradient(low='white', high='red') + ggsize(500, 500) + ggtitle(\"Average Score\") + scale_x_discrete_reversed() + scale_y_discrete()\n",
    ", 500, 0)\n",
    "\n",
    "bunch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/inkompotato/itu/nlp-project/lets-plot-images/results.html'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ggsave(bunch, \"results.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32717386921c27a635de762f5210753becc6f37ba8fb6494d8fab72c3a611423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
