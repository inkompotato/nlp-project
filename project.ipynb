{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, RobertaForQuestionAnswering, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    name: str\n",
    "    tokenizer: object\n",
    "    model: object\n",
    "\n",
    "    def __init__(self, name: str, tokenizer: object = None, model: object = None):\n",
    "        self.name = name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Model(name={self.name})\"\n",
    "\n",
    "    def set_tokenizer(self, tokenizer: object):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def set_model(self, model: object):\n",
    "        self.model = model\n",
    "\n",
    "    def to_pipeline(self):\n",
    "        return pipeline(\"fill-mask\", model=self.model, tokenizer=self.tokenizer, device=0, top_k = 1)\n",
    "\n",
    "    def get_collator(self):\n",
    "        return DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "\tdef __init__(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number =0\n",
    "\t\tself.str = 'perplexity'\n",
    "\t\tself.metric_scores = {}\n",
    "\n",
    "\t# cross entropy loss\n",
    "\tdef score(self, loss):\n",
    "\t\tself.sum += loss\n",
    "\t\t# should be number of sentences\n",
    "\t\tself.number += 1\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.sum = 0\n",
    "\t\tself.number = 0\n",
    "\n",
    "\tdef get_score(self):\n",
    "\t\tif self.sum ==0:\n",
    "\t\t\tself.metric_scores[self.str] = 0.0\n",
    "\t\tself.metric_scores[self.str] = math.exp(self.sum / self.number)\n",
    "\t\tself.metric_scores[\"sum\"] = self.str\n",
    "\t\treturn self.metric_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "models = {\n",
    "    \"de\" : Model(\"uklfr/gottbert-base\"),\n",
    "    \"nl\" : Model(\"pdelobelle/robbert-v2-dutch-base\"),\n",
    "    \"se\" : Model(\"birgermoell/roberta-swedish\"),\n",
    "    \"dk\" : Model(\"DDSC/roberta-base-danish\"),\n",
    "    # \"es\" : Model(\"bertin-project/bertin-roberta-base-spanish\"),\n",
    "}\n",
    "\n",
    "for language, model in tqdm(models.items()):\n",
    "    # get model & tokenizer from huggingface\n",
    "    model.set_tokenizer(AutoTokenizer.from_pretrained(model.name, model_max_length=512))\n",
    "    model.set_model(AutoModelForMaskedLM.from_pretrained(model.name).to(\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipelines\n",
    "\n",
    "pipelines = {k: v.to_pipeline() for k, v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load datasets\n",
    "\n",
    "def load_dataset_local(dataset_name):\n",
    "    with open(f\"wiki_data/{dataset_name}.txt\") as f:\n",
    "        return f.read().splitlines()\n",
    "\n",
    "datasets = {\n",
    "    \"nl\" : load_dataset_local(\"nlwiki_sentences\"),\n",
    "    \"dk\" : load_dataset_local(\"dawiki_sentences\"),\n",
    "    \"se\" : load_dataset_local(\"svwiki_sentences\"),\n",
    "    \"de\" : load_dataset_local(\"dewiki_sentences\"),\n",
    "    # \"es\" : load_dataset_local(\"eswiki_sentences\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "# tokenize each dataset for each model\n",
    "for model_language, pipeline in tqdm(pipelines.items()):\n",
    "\n",
    "    for dataset_language, dataset in datasets.items():\n",
    "        # raw data\n",
    "        raw_data = dataset\n",
    "        # split into sentences\n",
    "        # TODO: permorm additional data cleaning\n",
    "        sentences = [sentence.split(\" \") for text in raw_data for sentence in text.split(\".\")]\n",
    "        masked_words = []     \n",
    "        # mask one word in each sentence\n",
    "        for sentence in sentences:\n",
    "            # get random index\n",
    "            index = random.randint(0, len(sentence) - 1)\n",
    "            # mask word\n",
    "            masked_words.append(sentence[index])\n",
    "            sentence[index] = pipeline.tokenizer.mask_token\n",
    "\n",
    "        sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "        \n",
    "        try:\n",
    "            predictions = pipeline(sentences)\n",
    "        except RuntimeError:\n",
    "            print(f\"Error with {model_language} and {dataset_language}\")\n",
    "            continue\n",
    "\n",
    "        # get average score\n",
    "        softmax_scores = [prediction[0][\"score\"] for prediction in predictions]\n",
    "        predicted_sentences = [prediction[0][\"sequence\"] for prediction in predictions]\n",
    "        predicted_words = [prediction[0][\"token_str\"] for prediction in predictions]\n",
    "        # get vector for masked and predicted word\n",
    "        masked_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=True) for token in masked_words]\n",
    "        predict_index = [pipelines[dataset_language].tokenizer.encode(token, add_special_tokens=True) for token in predicted_words]\n",
    "        unknown_index = pipelines[dataset_language].tokenizer.encode(pipelines[dataset_language].tokenizer.pad_token, add_special_tokens=True)\n",
    "        word_pairs[f\"{model_language}-{dataset_language}\"] = [masked_words, predicted_words]\n",
    "        nr_unks = []\n",
    "        ul = len(unknown_index)\n",
    "        for i in range(len(masked_index)):\n",
    "            ml = len(masked_index[i])\n",
    "            pl = len(predict_index[i])\n",
    "            diff = abs(ml - pl)\n",
    "            repeats = diff // ul\n",
    "            nr_unks.append(repeats + 1)\n",
    "            if ml < pl and diff % ul == 0:\n",
    "                # diff is a multiple of ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index * repeats\n",
    "            elif ml < pl and diff % ul != 0 and diff < ul:\n",
    "                # diff is smaller than ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index[:diff]\n",
    "            elif ml < pl and diff % ul != 0 and diff >= ul:\n",
    "                # diff is larger than ul and not a multiple of ul\n",
    "                masked_index[i] = masked_index[i] + unknown_index * repeats + unknown_index[:diff % ul]\n",
    "            elif ml > pl and diff % ul == 0:\n",
    "                predict_index[i] = predict_index[i] + unknown_index * repeats\n",
    "            elif ml > pl and diff % ul != 0 and diff < ul:\n",
    "                predict_index[i] = predict_index[i] + unknown_index[:diff]\n",
    "            elif ml > pl and diff % ul != 0 and diff >= ul:\n",
    "                predict_index[i] = predict_index[i] + unknown_index * repeats + unknown_index[:diff % ul]\n",
    "\n",
    "        # calculate cosine similarity between predicted words and masked words\n",
    "        cosine_similarities = []\n",
    "        for i in range(len(masked_index)):\n",
    "            if masked_index[i] == [] or predict_index[i] == []:\n",
    "                cosine_similarities.append(np.array(0))\n",
    "                continue\n",
    "            # get vector for masked word\n",
    "            masked_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(masked_index[i]).to(\"cuda\"))\n",
    "            # get vector for predicted word\n",
    "            predict_vector = pipelines[dataset_language].model.roberta.embeddings.word_embeddings(torch.tensor(predict_index[i]).to(\"cuda\"))\n",
    "            # if masked_vector.shape[0] < predict_vector.shape[0]:\n",
    "            #     # extend with unk_vector until shape is the same\n",
    "            #     masked_vector = torch.nn.functional.pad(masked_vector, (0, 0, 0, predict_vector.shape[0] - masked_vector.shape[0]))\n",
    "            # elif masked_vector.shape[0] > predict_vector.shape[0]:\n",
    "            #     predict_vector = torch.nn.functional.pad(predict_vector, (0, 0, 0, masked_vector.shape[0] - predict_vector.shape[0]))\n",
    "\n",
    "            # calculate cosine similarity using torch\n",
    "            cos_sim = torch.nn.functional.cosine_similarity(masked_vector, predict_vector, dim=1).cpu().detach().numpy()\n",
    "            cosine_similarities.append(np.mean(cos_sim))\n",
    "\n",
    "        # calculate perplexity\n",
    "        perplexity_scores = perplexity.compute(predictions=predicted_words, references=masked_words, model_id=models[dataset_language].name)\n",
    "        perplexity_score = np.mean(perplexity_scores[\"perplexities\"]) / len(masked_words)\n",
    "        # mean cosine similarity\n",
    "        # flatten cos sim\n",
    "        # cosine_similarities = [item for sublist in cosine_similarities for item in sublist]\n",
    "        mean_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "        average_score = np.mean(softmax_scores)\n",
    "        median_score = np.median(softmax_scores)\n",
    "        print(f\"{model_language} - {dataset_language}: {average_score}, {mean_cosine_similarity}, {perplexity_score}\")\n",
    "        results[(model_language, dataset_language)] = [average_score, median_score, mean_cosine_similarity, perplexity_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.choice(range(0, len(sentences)))\n",
    "masked_words[idx], sentences[idx], predicted_sentences[idx], softmax_scores[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(word_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_df = []\n",
    "for (model_language, dataset_language), score in results.items():\n",
    "    to_df.append({\n",
    "        \"model_language\" : model_language,\n",
    "        \"dataset_language\" : dataset_language,\n",
    "        \"avg_score\" : score[0],\n",
    "        \"median_score\" : score[1],\n",
    "        \"cos_simil\" : score[2],\n",
    "        \"perplexity\" : score[3]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ranking\n",
    "df[\"rank\"] = df.groupby(\"dataset_language\")[\"cos_simil\"].rank(ascending=True)\n",
    "df.sort_values(by=[\"dataset_language\", \"model_language\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lets_plot import *\n",
    "LetsPlot.setup_html()\n",
    "\n",
    "bunch = GGBunch()\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='rank'), data=df) + scale_fill_gradient(low='white', high='blue') + ggsize(500, 500) + ggtitle(\"Cosine Similarity\") + scale_x_discrete_reversed() + scale_y_discrete()\n",
    ", 0, 0)\n",
    "bunch.add_plot(ggplot() + geom_tile(aes(x='model_language', y='dataset_language', fill='avg_score'), data=df) + scale_fill_gradient(low='white', high='red') + ggsize(500, 500) + ggtitle(\"Average Score\") + scale_x_discrete_reversed() + scale_y_discrete()\n",
    ", 500, 0)\n",
    "\n",
    "bunch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggsave(bunch, \"results.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32717386921c27a635de762f5210753becc6f37ba8fb6494d8fab72c3a611423"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
